{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20191017-194013\n",
      "torch.Size([29049, 32])\n",
      "torch.Size([2305, 32])\n",
      "torch.Size([7376, 10])\n",
      "torch.Size([82430, 1])\n",
      "10/17 07:40:22 PM param size: 4810000\n",
      "10/17 07:40:22 PM initial genotype:\n",
      "10/17 07:40:22 PM Genotype(recurrent=[('identity', 0), ('relu', 1), ('identity', 0), ('identity', 2), ('sigmoid', 4), ('tanh', 0), ('sigmoid', 1), ('identity', 6)], concat=range(1, 9))\n",
      "10/17 07:40:22 PM Args: Namespace(alpha=0, arch_lr=0.003, arch_wdecay=0.001, batch_size=32, beta=0.001, bptt=35, clip=0.25, continue_train=False, cuda=True, data='../data/penn/', dropout=0.75, dropoute=0, dropouth=0.25, dropouti=0.2, dropoutx=0.75, emsize=300, epochs=300, gpu=0, log_interval=50, lr=20, max_seq_len_delta=20, nhid=300, nhidlast=300, nonmono=5, save='search-EXP-20191017-194013', seed=3, single_gpu=True, small_batch_size=32, unrolled=False, wdecay=5e-07)\n",
      "10/17 07:40:22 PM Model total parameters: 4810000\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os, sys, glob\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from architect import Architect\n",
    "\n",
    "from genotypes import PRIMITIVES, STEPS, CONCAT, Genotype\n",
    "\n",
    "import genotypes\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import data\n",
    "import model_search as model\n",
    "\n",
    "import inspect\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank/WikiText2 Language Model')\n",
    "parser.add_argument('--data', type=str, default='../data/penn/',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--emsize', type=int, default=300,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=300,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nhidlast', type=int, default=300,\n",
    "                    help='number of hidden units for the last rnn layer')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.75,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--dropouth', type=float, default=0.25,\n",
    "                    help='dropout for hidden nodes in rnn layers (0 = no dropout)')\n",
    "parser.add_argument('--dropoutx', type=float, default=0.75,\n",
    "                    help='dropout for input nodes in rnn layers (0 = no dropout)')\n",
    "parser.add_argument('--dropouti', type=float, default=0.2,\n",
    "                    help='dropout for input embedding layers (0 = no dropout)')\n",
    "parser.add_argument('--dropoute', type=float, default=0,\n",
    "                    help='dropout to remove words from embedding layer (0 = no dropout)')\n",
    "parser.add_argument('--seed', type=int, default=3,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--nonmono', type=int, default=5,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_false',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str,  default='EXP',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--alpha', type=float, default=0,\n",
    "                    help='alpha L2 regularization on RNN activation (alpha = 0 means no regularization)')\n",
    "parser.add_argument('--beta', type=float, default=1e-3,\n",
    "                    help='beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)')\n",
    "parser.add_argument('--wdecay', type=float, default=5e-7,\n",
    "                    help='weight decay applied to all weights')\n",
    "parser.add_argument('--continue_train', action='store_true',\n",
    "                    help='continue train from a checkpoint')\n",
    "parser.add_argument('--small_batch_size', type=int, default=-1,\n",
    "                    help='the batch size for computation. batch_size should be divisible by small_batch_size.\\\n",
    "                     In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "                     until batch_size is reached. An update step is then performed.')\n",
    "parser.add_argument('--max_seq_len_delta', type=int, default=20,\n",
    "                    help='max sequence length')\n",
    "parser.add_argument('--single_gpu', default=True, action='store_false', \n",
    "                    help='use single GPU')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='GPU device to use')\n",
    "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_wdecay', type=float, default=1e-3,\n",
    "                    help='weight decay for the architecture encoding alpha')\n",
    "parser.add_argument('--arch_lr', type=float, default=3e-3,\n",
    "                    help='learning rate for the architecture encoding alpha')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "if args.nhidlast < 0:\n",
    "    args.nhidlast = args.emsize\n",
    "if args.small_batch_size < 0:\n",
    "    args.small_batch_size = args.batch_size\n",
    "\n",
    "if not args.continue_train:\n",
    "    args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        cudnn.benchmark = True\n",
    "        cudnn.enabled=True\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "search_data = batchify(corpus.valid, args.batch_size, args)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, args)\n",
    "test_data = batchify(corpus.test, test_batch_size, args)\n",
    "\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "if args.continue_train:\n",
    "    model = torch.load(os.path.join(args.save, 'model.pt'))\n",
    "else:\n",
    "    model = model.RNNModelSearch(ntokens, args.emsize, args.nhid, args.nhidlast, \n",
    "                       args.dropout, args.dropouth, args.dropoutx, args.dropouti, args.dropoute)\n",
    "\n",
    "size = 0\n",
    "for p in model.parameters():\n",
    "    size += p.nelement()\n",
    "logging.info('param size: {}'.format(size))\n",
    "logging.info('initial genotype:')\n",
    "logging.info(model.genotype())\n",
    "\n",
    "if args.cuda:\n",
    "    if args.single_gpu:\n",
    "        parallel_model = model.cuda()\n",
    "    else:\n",
    "        parallel_model = nn.DataParallel(model, dim=1).cuda()\n",
    "else:\n",
    "    parallel_model = model\n",
    "architect = Architect(parallel_model, args)\n",
    "\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "logging.info('Args: {}'.format(args))\n",
    "logging.info('Model total parameters: {}'.format(total_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_model_arch(model, arch):\n",
    "    for rnn in model.rnns:\n",
    "        rnn.genotype = arch\n",
    "        \n",
    "def set_model_weights(model, weights):\n",
    "    model.weights = Variable(weights.cuda(), requires_grad=True)\n",
    "    model._arch_parameters = [model.weights]\n",
    "    \n",
    "\n",
    "def random_arch_gene():\n",
    "    n_nodes = genotypes.STEPS\n",
    "    n_ops = len(genotypes.PRIMITIVES)\n",
    "    arch_gene = []\n",
    "    arch=[]\n",
    "    for i in range(n_nodes):\n",
    "        op = np.random.choice(range(1,n_ops))\n",
    "        node_in = np.random.choice(range(i+1))\n",
    "        arch_gene.append((op, node_in))\n",
    "        arch.append((genotypes.PRIMITIVES[op], node_in))\n",
    "    concat = range(1,9)\n",
    "    genotype = genotypes.Genotype(recurrent=arch, concat=concat)\n",
    "    return arch_gene,arch\n",
    "\n",
    "\n",
    "\n",
    "def _genotype(weights):\n",
    "    STEPS=8\n",
    "    def _parse(probs):\n",
    "        gene = []\n",
    "        start = 0\n",
    "        for i in range(STEPS):\n",
    "            end = start + i + 1\n",
    "            W = probs[start:end].copy()\n",
    "            j = sorted(range(i + 1), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[0]\n",
    "            k_best = None\n",
    "            for k in range(len(W[j])):\n",
    "                if k != PRIMITIVES.index('none'):\n",
    "                    if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                        k_best = k\n",
    "            gene.append((PRIMITIVES[k_best], j))\n",
    "            start = end\n",
    "        return gene\n",
    "\n",
    "    gene = _parse(F.softmax(weights, dim=-1).data.cpu().numpy())\n",
    "    genotype = Genotype(recurrent=gene, concat=range(STEPS+1)[-CONCAT:])\n",
    "    return genotype\n",
    "\n",
    "\n",
    "\n",
    "def _parse_gene(weights):\n",
    "    STEPS=8\n",
    "    gene = []\n",
    "    start = 0\n",
    "    weights_t=F.softmax(weights, dim=-1).data.cpu().numpy()\n",
    "    for i in range(STEPS):\n",
    "        end = start + i + 1\n",
    "        W = weights_t[start:end].copy()\n",
    "        j = sorted(range(i + 1), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[0]\n",
    "        k_best = None\n",
    "        for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "                if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                    k_best = k\n",
    "        gene.append((k_best, j))\n",
    "        start = end\n",
    "    return gene\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "def get_weights_from_arch(arch_gene):\n",
    "    n_nodes = 8\n",
    "    n_ops = 5\n",
    "    weights = torch.zeros(sum([i+1 for i in range(n_nodes)]), n_ops)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(n_nodes):\n",
    "        op = arch_gene[i][0]\n",
    "        node_in = arch_gene[i][1]\n",
    "        ind = offset + node_in\n",
    "        weights[ind, op] = 5\n",
    "        offset += (i+1)\n",
    "\n",
    "    weights = torch.autograd.Variable(weights.cuda(), requires_grad=False)\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_arch_generate():\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = 4####model._step\n",
    "\n",
    "    arch_gene = []\n",
    "    for i in range(n_nodes):\n",
    "        ops = np.random.choice(range(num_ops), 2)\n",
    "        nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "        arch_gene.extend([(ops[0],nodes_in_normal[0]), (ops[1],nodes_in_normal[1])])\n",
    "    return arch_gene  \n",
    "\n",
    "def cal_arch_dis(arch1,arch2):#################small distance more similar\n",
    "    dis=8\n",
    "    n_nodes=4######genotypes.STEPS\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        if arch1[2*i]==arch2[2*i]:\n",
    "            dis=dis-1\n",
    "        elif arch1[2*i]==arch2[2*i+1]:\n",
    "            dis=dis-1\n",
    "        if arch1[2*i+1]==arch2[2*i+1]:\n",
    "            dis=dis-1\n",
    "        elif arch1[2*i+1]==arch2[2*i]:\n",
    "            dis=dis-1                      \n",
    "    dis=dis/8\n",
    "    return dis \n",
    "\n",
    "\n",
    "\n",
    "def cal_diver_score(arch,archive):\n",
    "    n=len(archive)\n",
    "    dis=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        dis[i]=cal_arch_dis(arch,archive[i])\n",
    "        \n",
    "    sort_dis=np.sort(dis)\n",
    "\n",
    "    diver_score=np.mean(sort_dis[0:10])###################################k=10 for knn\n",
    "    \n",
    "    return diver_score\n",
    " \n",
    "\n",
    "    \n",
    "def diver_arch_generate(arch_archive):############randomly genrate architecture and get the best one\n",
    "    ini_diver_score=0\n",
    "    arch_g=random_arch_generate()\n",
    "    for i in range(10):##################\n",
    "        arch=random_arch_generate()         \n",
    "        diver_score=cal_diver_score(arch,arch_archive)#########kernel metric, the samller the better\n",
    "        if diver_score>ini_diver_score:\n",
    "            arch_g=arch\n",
    "            ini_diver_score=diver_score\n",
    "            \n",
    "    return arch_g\n",
    "\n",
    "\n",
    "def diver_arch_replace(index,arch_archive,archive_recent):############randomly generate architecture to repalce\n",
    "    arch_compar=arch_archive[index]\n",
    "    a=np.arange(0,index)\n",
    "    b=np.arange(index+1,len(arch_archive))\n",
    "    index_remain=np.append(a,b)\n",
    "    \n",
    "    arch_archive_remain=[arch_archive[j] for j in index_remain]\n",
    "    \n",
    "    ini_diver_score=cal_diver_score(arch_compar,arch_archive_remain)\n",
    "    for i in range(len(archive_recent)):######################################\n",
    "        arch=archive_recent[i]\n",
    "        diver_score=cal_diver_score(arch,arch_archive_remain)\n",
    "        if diver_score>ini_diver_score:\n",
    "            arch_compar=arch\n",
    "            ini_diver_score=diver_score\n",
    "            \n",
    "    return arch_compar\n",
    "\n",
    "\n",
    "def find_similar_arch(arch,archive):\n",
    "    dis=np.zeros(len(archive))   \n",
    "    \n",
    "    for i in range(len(archive)):\n",
    "        dis[i]=cal_arch_dis(arch,archive[i])################\n",
    "\n",
    "    m=np.argsort(dis)\n",
    "    index=m[0]\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def arch_archive_update(arch_gene,arch_archive,archive_recent):\n",
    "    store_num=8\n",
    "    if len(arch_archive)==store_num:\n",
    "        ind_arch_norm_replace=find_similar_arch(arch_gene,arch_archive)\n",
    "        arch_archive[ind_arch_norm_replace]=diver_arch_replace(ind_arch_norm_replace,arch_archive,archive_recent)        \n",
    "    else:\n",
    "        normal_arch=diver_arch_generate(arch_archive)\n",
    "        arch_archive.append(normal_arch)\n",
    "    return arch_archive\n",
    "\n",
    "\n",
    "\n",
    "def cal_loss_archive(arch_gene,arch_archive_new,model_save,input,target,criterion):\n",
    "    loss_arch=0\n",
    "    \n",
    "    for i in range(np.int(len(arch_archive_new)/2)):\n",
    "        w1=1-cal_arch_dis(arch_gene[0],arch_archive_new[2*i])##############################\n",
    "        w2=1-cal_arch_dis(arch_gene[1],arch_archive_new[2*i+1])\n",
    "        w=(w1+w2)/2\n",
    "        model_save_save=copy.deepcopy(model_save)        \n",
    "        model_weights=get_weights_from_arch(arch_archive_new[2*i:2*i+2])  \n",
    "        model_save_save=set_model_weights(model_save_save,model_weights)\n",
    "        \n",
    "        logits = model_save_save(input)        \n",
    "        loss=criterion(logits, target)\n",
    "        loss_arch=w*(loss_arch+loss.item())\n",
    "        del model_save_save\n",
    "    loss_archive=(loss_arch*2)/len(arch_archive_new)\n",
    "    del model_save\n",
    "    return loss_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_model_arch(model, arch):\n",
    "    for rnn in model.rnns:\n",
    "        rnn.genotype = arch\n",
    "        \n",
    "def set_model_weights(model, weights):\n",
    "    model.weights = Variable(weights.cuda(), requires_grad=True)\n",
    "    model._arch_parameters = [model.weights]\n",
    "    \n",
    "\n",
    "def random_arch_gene():\n",
    "    n_nodes = genotypes.STEPS\n",
    "    n_ops = len(genotypes.PRIMITIVES)\n",
    "    arch_gene = []\n",
    "    arch=[]\n",
    "    for i in range(n_nodes):\n",
    "        op = np.random.choice(range(1,n_ops))\n",
    "        node_in = np.random.choice(range(i+1))\n",
    "        arch_gene.append((op, node_in))\n",
    "        arch.append((genotypes.PRIMITIVES[op], node_in))\n",
    "    concat = range(1,9)\n",
    "    genotype = genotypes.Genotype(recurrent=arch, concat=concat)\n",
    "    return arch_gene,arch\n",
    "\n",
    "\n",
    "\n",
    "def _genotype(weights):\n",
    "    STEPS=8\n",
    "    def _parse(probs):\n",
    "        gene = []\n",
    "        start = 0\n",
    "        for i in range(STEPS):\n",
    "            end = start + i + 1\n",
    "            W = probs[start:end].copy()\n",
    "            j = sorted(range(i + 1), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[0]\n",
    "            k_best = None\n",
    "            for k in range(len(W[j])):\n",
    "                if k != PRIMITIVES.index('none'):\n",
    "                    if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                        k_best = k\n",
    "            gene.append((PRIMITIVES[k_best], j))\n",
    "            start = end\n",
    "        return gene\n",
    "\n",
    "    gene = _parse(F.softmax(weights, dim=-1).data.cpu().numpy())\n",
    "    genotype = Genotype(recurrent=gene, concat=range(STEPS+1)[-CONCAT:])\n",
    "    return genotype\n",
    "\n",
    "\n",
    "\n",
    "def _parse_gene(weights):\n",
    "    STEPS=8\n",
    "    gene = []\n",
    "    start = 0\n",
    "    weights_t=F.softmax(weights, dim=-1).data.cpu().numpy()\n",
    "    for i in range(STEPS):\n",
    "        end = start + i + 1\n",
    "        W = weights_t[start:end].copy()\n",
    "        j = sorted(range(i + 1), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[0]\n",
    "        k_best = None\n",
    "        for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "                if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                    k_best = k\n",
    "        gene.append((k_best, j))\n",
    "        start = end\n",
    "    return gene\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_arch_from_gene(arch_gene):\n",
    "    n_nodes = 8\n",
    "    n_ops = 5 \n",
    "    arch=[]\n",
    "    for i in range(n_nodes):\n",
    "        op = arch_gene[i][0]\n",
    "        node_in = arch_gene[i][1]    \n",
    "        arch.append((genotypes.PRIMITIVES[op], node_in))\n",
    "    concat = range(1,9)\n",
    "    genotype = genotypes.Genotype(recurrent=arch, concat=concat)\n",
    "    \n",
    "    return genotype\n",
    "\n",
    "\n",
    "def get_weights_from_arch(arch_gene):\n",
    "    n_nodes = 8\n",
    "    n_ops = 5\n",
    "    weights = torch.zeros(sum([i+1 for i in range(n_nodes)]), n_ops)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(n_nodes):\n",
    "        op = arch_gene[i][0]\n",
    "        node_in = arch_gene[i][1]\n",
    "        ind = offset + node_in\n",
    "        weights[ind, op] = 5\n",
    "        offset += (i+1)\n",
    "\n",
    "    weights = torch.autograd.Variable(weights.cuda(), requires_grad=False)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data_source, batch_size=10):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i, args, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = parallel_model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,epoch,arch_archive,archive_recent):\n",
    "    assert args.batch_size % args.small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "        \n",
    "    #temp= opt.initial_temp * np.exp(-opt.anneal_rate * epoch)\n",
    "    temp= 2.5 * np.exp(-0.00003* epoch)\n",
    "    #temperature=torch.FloatTensor([temp])\n",
    "    \n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\n",
    "    hidden_valid = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        # seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "        # # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        # seq_len = min(seq_len, args.bptt + args.max_seq_len_delta)\n",
    "        seq_len = int(bptt)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / args.bptt\n",
    "        model.train()\n",
    "\n",
    "        data_valid, targets_valid = get_batch(search_data, i % (search_data.size(0) - 1), args)\n",
    "        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, args.small_batch_size, 0\n",
    "        while start < args.batch_size:\n",
    "            \n",
    "            weights_save=model.weights\n",
    "            \n",
    "            #arch,weights_gumble=sample_arch_from_weights(weights_save,temp)  \n",
    "            #arch_gene=_parse_gene(weights_gumble)\n",
    "            \n",
    "            arch_gene,arch=random_arch_gene()                   \n",
    "            weight_arch=get_weights_from_arch(arch_gene)\n",
    "            \n",
    "            set_model_arch(model, arch)\n",
    "            set_model_weights(model, weight_arch)\n",
    "            \n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "            cur_data_valid, cur_targets_valid = data_valid[:, start: end], targets_valid[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "\n",
    "\n",
    "            # assuming small_batch_size = batch_size so we don't accumulate gradients\n",
    "            optimizer.zero_grad()\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = parallel_model(cur_data, hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "\n",
    "            loss = raw_loss\n",
    "            # Activiation Regularization\n",
    "            if args.alpha > 0:\n",
    "                loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal Activation Regularization (slowness)\n",
    "            loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= args.small_batch_size / args.batch_size\n",
    "            total_loss += raw_loss.data * args.small_batch_size / args.batch_size\n",
    "            \n",
    "            archive_recent.extend([arch_gene])\n",
    "            archive_recent=archive_recent[-20:]\n",
    "            \n",
    "            arch_archive=arch_archive_update(arch_gene,arch_archive,archive_recent)\n",
    "            \n",
    "            loss_sr=0\n",
    "            for i in range(len(arch_archive)):\n",
    "                model_save_r=copy.deepcopy(model)\n",
    "                \n",
    "                arch_gene_r=arch_archive[i]     \n",
    "                weight_arch_r=get_weights_from_arch(arch_gene_r)\n",
    "                arch_r=get_arch_from_gene(arch_gene_r)\n",
    "\n",
    "\n",
    "                set_model_arch(model_save_r, arch_r)\n",
    "                set_model_weights(model_save_r, weight_arch_r)\n",
    "\n",
    "                cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "                cur_data_valid, cur_targets_valid = data_valid[:, start: end], targets_valid[:, start: end].contiguous().view(-1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "\n",
    "                log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = parallel_model(cur_data, hidden[s_id], return_h=True)\n",
    "                raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "\n",
    "                loss_r = raw_loss\n",
    "                # Activiation Regularization\n",
    "                if args.alpha > 0:\n",
    "                    loss_r = loss_r + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "                # Temporal Activation Regularization (slowness)\n",
    "                loss_r = loss_r + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "                loss_r *= args.small_batch_size / args.batch_size\n",
    "                \n",
    "                loss_sr=loss_r+loss_r\n",
    "            loss_sr=loss_sr/(len(arch_archive))\n",
    "            \n",
    "            loss_f=0.5*loss+0.5*loss_sr\n",
    "       \n",
    "            \n",
    "            loss_f.backward()\n",
    "\n",
    "            \n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + args.small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            logging.info(parallel_model.genotype())\n",
    "            #print(F.softmax(parallel_model.weights, dim=-1))\n",
    "            cur_loss = total_loss[0] / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging.info('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "\n",
    "if args.continue_train:\n",
    "    optimizer_state = torch.load(os.path.join(args.save, 'optimizer.pt'))\n",
    "    if 't0' in optimizer_state['param_groups'][0]:\n",
    "        optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stored_loss = 100000000\n",
    "archive_recent=[]\n",
    "arch_archive=[]\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model,epoch,arch_archive,archive_recent)\n",
    "\n",
    "    val_loss = evaluate(val_data, eval_batch_size)\n",
    "    logging.info('-' * 89)\n",
    "    logging.info('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                       val_loss, math.exp(val_loss)))\n",
    "    logging.info('-' * 89)\n",
    "\n",
    "    if val_loss < stored_loss:\n",
    "        save_checkpoint(model, optimizer, epoch, args.save)\n",
    "        logging.info('Saving Normal!')\n",
    "        stored_loss = val_loss\n",
    "\n",
    "    best_val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
