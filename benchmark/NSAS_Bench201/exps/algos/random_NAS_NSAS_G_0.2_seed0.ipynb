{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "sys.path.append(str('/home/taoliu/Data/NAS-Projects-master/lib'))\n",
    "sys.path.append(str('/home/taoliu/Data/NAS-Projects-master/configs'))\n",
    "from config_utils import load_config, dict2config, configure2str\n",
    "from datasets     import get_datasets, SearchDataset\n",
    "from procedures   import prepare_seed, prepare_logger, save_checkpoint, copy_checkpoint, get_optim_scheduler\n",
    "from utils        import get_model_infos, obtain_accuracy\n",
    "from log_utils    import AverageMeter, time_string, convert_secs2time\n",
    "from models       import get_cell_based_tiny_net, get_search_spaces\n",
    "from nas_102_api  import NASBench102API as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument('--data_path',          type=str,   default= '/data/taoliu/ENNAS_benchmark102/data/cifar-10-batches-py',    help='Path to dataset')\n",
    "parser.add_argument('--dataset',            type=str,   default= 'cifar10',choices=['cifar10', 'cifar100', 'ImageNet16-120'], help='Choose between Cifar10/100 and ImageNet-16.')\n",
    "# channels and number-of-cells\n",
    "parser.add_argument('--search_space_name',  type=str,   default= 'nas-bench-102', help='The search space name.')\n",
    "parser.add_argument('--config_path',        type=str,   default= '/home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/algos/RANDOM.config'  ,help='The path to the configuration.')\n",
    "parser.add_argument('--max_nodes',          type=int,   default= 4 , help='The maximum number of nodes.')\n",
    "parser.add_argument('--channel',            type=int,   default= 16 , help='The number of channels.')\n",
    "parser.add_argument('--num_cells',          type=int,   default= 5, help='The number of cells in one stage.')\n",
    "parser.add_argument('--select_num',         type=int,   default= 100, help='The number of selected architectures to evaluate.')\n",
    "# log\n",
    "parser.add_argument('--workers',            type=int,   default=2, help='number of data loading workers (default: 2)')\n",
    "parser.add_argument('--save_dir',           type=str,   default='./RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10',help='Folder to save checkpoints and log.')\n",
    "parser.add_argument('--arch_nas_dataset',   type=str,   default='/home/taoliu/Data/NAS-Projects-master/NAS-Bench-102-v1_0-e61699.pth',help='The path to load the architecture dataset (tiny-nas-benchmark).')\n",
    "parser.add_argument('--print_freq',         type=int,   default=200,help='print frequency (default: 200)')\n",
    "parser.add_argument('--rand_seed',          type=int,   default=0,help='manual seed')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_compensate(pre_index,cur_index,pre_hardwts,cur_hardwts):\n",
    "    compen_index   = cur_index\n",
    "    for i in range(pre_index.shape[0]):\n",
    "        if pre_index[i][0].item()==cur_index[i][0].item():\n",
    "            a=np.random.randint(5)\n",
    "            while a==pre_index[i][0].item():\n",
    "                a=np.random.randint(5)\n",
    "            compen_index[i][0]=a              \n",
    "    one_h   = torch.zeros_like(cur_hardwts).scatter_(-1, compen_index, 1.0)\n",
    "    compen_hardwts = one_h - cur_hardwts.detach() + cur_hardwts\n",
    "    \n",
    "    return compen_index,compen_hardwts\n",
    "\n",
    "def random_gene(cur_index,cur_hardwts):\n",
    "    compen_index   = cur_index\n",
    "    for i in range(cur_index.shape[0]):        \n",
    "        a=np.random.randint(5)\n",
    "        compen_index[i][0]=a              \n",
    "    one_h   = torch.zeros_like(cur_hardwts).scatter_(-1, compen_index, 1.0)\n",
    "    compen_hardwts = one_h - cur_hardwts.detach() + cur_hardwts\n",
    "    \n",
    "    return compen_index,compen_hardwts\n",
    "\n",
    "\n",
    "def diver_gene(cur_index,cur_hardwts):\n",
    "    compen_index   = cur_index\n",
    "    for i in range(cur_index.shape[0]):\n",
    "        a=np.random.randint(5)\n",
    "        while a==cur_index[i][0].item():\n",
    "            a=np.random.randint(5)\n",
    "        compen_index[i][0]=a              \n",
    "    one_h   = torch.zeros_like(cur_hardwts).scatter_(-1, compen_index, 1.0)\n",
    "    compen_hardwts = one_h - cur_hardwts.detach() + cur_hardwts\n",
    "    \n",
    "    return compen_index,compen_hardwts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_func(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    arch_pre,genotypes_pre,operations_pre=network.module.random_genotype( False )\n",
    "    for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(xloader):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "    \n",
    "        # update the weights\n",
    "        arch,genotypes,operations=network.module.random_genotype( True )\n",
    "        \n",
    "                \n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        loss = criterion(logits, base_targets)\n",
    "        \n",
    "        arch_random,genotypes_random,operations_random=network.module.random_genotype( False ) \n",
    "        arch_random=network.module.set_genotype(arch_random)\n",
    "        _, logits = network(base_inputs)\n",
    "        random_loss = criterion(logits, base_targets) \n",
    "        \n",
    "        \n",
    "        arch_diver,genotypes_diver,operations_diver= network.module.arch_compensate(operations_random,operations)\n",
    "        \n",
    "        arch_diver=network.module.set_genotype(arch_diver)\n",
    "        _, logits = network(base_inputs)\n",
    "        diver_loss = criterion(logits, base_targets)        \n",
    "         \n",
    "        \n",
    "        arch=network.module.set_genotype(arch)\n",
    "        \n",
    "        \n",
    "        base_loss=0.2*loss+0.4*random_loss+0.4*diver_loss\n",
    "        \n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "            # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(logits.data, base_targets.data, topk=(1, 5))\n",
    "        base_losses.update(base_loss.item(),  base_inputs.size(0))\n",
    "        base_top1.update  (base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update  (base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "        #####current to pre\n",
    "        arch_pre,genotypes_pre,operations_pre=arch,genotypes,operations\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = '*SEARCH* ' + time_string() + ' [{:}][{:03d}/{:03d}]'.format(epoch_str, step, len(xloader))\n",
    "            Tstr = 'Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})'.format(batch_time=batch_time, data_time=data_time)\n",
    "            Wstr = 'Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]'.format(loss=base_losses, top1=base_top1, top5=base_top5)\n",
    "            logger.log(Sstr + ' ' + Tstr + ' ' + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "\n",
    "def valid_func(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "          # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "       # prediction\n",
    "\n",
    "            network.module.random_genotype( True )\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "      # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(logits.data, arch_targets.data, topk=(1, 5))\n",
    "            arch_losses.update(arch_loss.item(),  arch_inputs.size(0))\n",
    "            arch_top1.update  (arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update  (arch_prec5.item(), arch_inputs.size(0))\n",
    "      # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg\n",
    "\n",
    "def search_find_best(valid_loader, network, criterion, select_num):\n",
    "    best_acc = 0\n",
    "    for iarch in range(select_num):\n",
    "        arch, genotypes,operations = network.module.random_genotype( True )\n",
    "        valid_a_loss, valid_a_top1, valid_a_top5  = valid_func(valid_loader, network, criterion)\n",
    "        if best_acc < valid_a_top1:\n",
    "            best_arch, best_acc = arch, valid_a_top1\n",
    "    return best_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : /home/taoliu/Data/NAS-Projects-master/NAS-Bench-102-v1_0-e61699.pth\n",
      "channel          : 16\n",
      "config_path      : /home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/algos/RANDOM.config\n",
      "data_path        : /data/taoliu/ENNAS_benchmark102/data/cifar-10-batches-py\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 0\n",
      "save_dir         : ./RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10\n",
      "search_space_name : nas-bench-102\n",
      "select_num       : 100\n",
      "workers          : 2\n",
      "Python  Version  : 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)  [GCC 7.2.0]\n",
      "Pillow  Version  : 5.1.0\n",
      "PyTorch Version  : 0.4.1\n",
      "cuDNN   Version  : 7102\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Load split file from /home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/cifar-split.txt\n",
      "/home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/algos/RANDOM.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, class_num=10, xshape=(1, 3, 32, 32))\n",
      "config : Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=391, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=100, current::epoch=0, iter=0.00, type=cosine, T-max=100, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "try to create the NAS-Bench-102 api from /home/taoliu/Data/NAS-Projects-master/NAS-Bench-102-v1_0-e61699.pth\n",
      "[2020-03-11 01:59:45] create API = NASBench102API(15625/15625 architectures) done\n",
      "=> loading checkpoint of the last-info 'RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10/seed-0-last-info.pth' start\n",
      "=> loading checkpoint of the last-info '{'epoch': 100, 'args': Namespace(arch_nas_dataset='/home/taoliu/Data/NAS-Projects-master/NAS-Bench-102-v1_0-e61699.pth', channel=16, config_path='/home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/algos/RANDOM.config', data_path='/data/taoliu/ENNAS_benchmark102/data/cifar-10-batches-py', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=0, save_dir='./RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10', search_space_name='nas-bench-102', select_num=100, workers=2), 'last_checkpoint': PosixPath('RANDOM_NSAS_G_0.2_seed0/output/search-cell-nas-bench-102-cifar10/checkpoint/seed-0-basic.pth')}' start with 100-th epoch.\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Pre-searching costs 0.0 s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04a3da334e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miarch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_genotype\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mvalid_a_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_a_top1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_a_top5\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalid_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final evaluation [{:02d}/{:02d}] : {:} : accuracy={:.2f}%, loss={:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miarch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_a_top1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_a_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_arch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvalid_a_top1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8facf3b63c24>\u001b[0m in \u001b[0;36mvalid_func\u001b[0;34m(xloader, network, criterion)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0march_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march_targets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0march_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0march_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0;31m# measure data loading time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ensure that the worker exits on process exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/taoliu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "#def main(xargs):\n",
    "xargs=args\n",
    "assert torch.cuda.is_available(), 'CUDA is not available.'\n",
    "torch.backends.cudnn.enabled   = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads( xargs.workers )\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)\n",
    "\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "if xargs.dataset == 'cifar10' or xargs.dataset == 'cifar100':\n",
    "    split_Fpath = '/home/taoliu/Data/NAS-Projects-master/configs/nas-benchmark/cifar-split.txt'\n",
    "    cifar_split = load_config(split_Fpath, None, None)\n",
    "    train_split, valid_split = cifar_split.train, cifar_split.valid\n",
    "    logger.log('Load split file from {:}'.format(split_Fpath))\n",
    "#elif xargs.dataset.startswith('ImageNet16'):\n",
    "#  # all_indexes = list(range(len(train_data))) ; random.seed(111) ; random.shuffle(all_indexes)\n",
    "#  # train_split, valid_split = sorted(all_indexes[: len(train_data)//2]), sorted(all_indexes[len(train_data)//2 :])\n",
    "#  # imagenet16_split = dict2config({'train': train_split, 'valid': valid_split}, None)\n",
    "#  # _ = configure2str(imagenet16_split, 'temp.txt')\n",
    "#  split_Fpath = 'configs/nas-benchmark/{:}-split.txt'.format(xargs.dataset)\n",
    "#  imagenet16_split = load_config(split_Fpath, None, None)\n",
    "#  train_split, valid_split = imagenet16_split.train, imagenet16_split.valid\n",
    "#  logger.log('Load split file from {:}'.format(split_Fpath))\n",
    "else:\n",
    "    raise ValueError('invalid dataset : {:}'.format(xargs.dataset))\n",
    "config = load_config(xargs.config_path, {'class_num': class_num, 'xshape': xshape}, logger)\n",
    "logger.log('config : {:}'.format(config))\n",
    "# To split data\n",
    "train_data_v2 = deepcopy(train_data)\n",
    "train_data_v2.transform = valid_data.transform\n",
    "valid_data    = train_data_v2\n",
    "search_data   = SearchDataset(xargs.dataset, train_data, train_split, valid_split)\n",
    "# data loader\n",
    "search_loader = torch.utils.data.DataLoader(search_data, batch_size=config.batch_size, shuffle=True , num_workers=xargs.workers, pin_memory=True)\n",
    "valid_loader  = torch.utils.data.DataLoader(valid_data, batch_size=config.batch_size, sampler=torch.utils.data.sampler.SubsetRandomSampler(valid_split), num_workers=xargs.workers, pin_memory=True)\n",
    "logger.log('||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}'.format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log('||||||| {:10s} ||||||| Config={:}'.format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces('cell', xargs.search_space_name)\n",
    "model_config = dict2config({'name': 'RANDOM', 'C': xargs.channel, 'N': xargs.num_cells,\n",
    "                          'max_nodes': xargs.max_nodes, 'num_classes': class_num,\n",
    "                          'space'    : search_space}, None)\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "logger.log('w-optimizer : {:}'.format(w_optimizer))\n",
    "logger.log('w-scheduler : {:}'.format(w_scheduler))\n",
    "logger.log('criterion   : {:}'.format(criterion))\n",
    "if xargs.arch_nas_dataset is None: api = None\n",
    "else                             : api = API(xargs.arch_nas_dataset)\n",
    "logger.log('{:} create API = {:} done'.format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = logger.path('info'), logger.path('model'), logger.path('best')\n",
    "network, criterion = torch.nn.DataParallel(search_model,device_ids=[0]).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists(): # automatically resume from previous checkpoint\n",
    "    logger.log(\"=> loading checkpoint of the last-info '{:}' start\".format(last_info))\n",
    "    last_info   = torch.load(last_info)\n",
    "    start_epoch = last_info['epoch']\n",
    "    checkpoint  = torch.load(last_info['last_checkpoint'])\n",
    "    genotypes   = checkpoint['genotypes']\n",
    "    valid_accuracies = checkpoint['valid_accuracies']\n",
    "    search_model.load_state_dict( checkpoint['search_model'] )\n",
    "    w_scheduler.load_state_dict ( checkpoint['w_scheduler'] )\n",
    "    w_optimizer.load_state_dict ( checkpoint['w_optimizer'] )\n",
    "    logger.log(\"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(last_info, start_epoch))\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {'best': -1}, {}\n",
    "\n",
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = time.time(), AverageMeter(), AverageMeter(), config.epochs + config.warmup\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = 'Time Left: {:}'.format( convert_secs2time(epoch_time.val * (total_epoch-epoch), True) )\n",
    "    epoch_str = '{:03d}-{:03d}'.format(epoch, total_epoch)\n",
    "    logger.log('\\n[Search the {:}-th epoch] {:}, LR={:}'.format(epoch_str, need_time, min(w_scheduler.get_lr())))\n",
    "\n",
    "# selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = search_func(search_loader, network, criterion, w_scheduler, w_optimizer, epoch_str, xargs.print_freq, logger)\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log('[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s'.format(epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum))\n",
    "    valid_a_loss , valid_a_top1 , valid_a_top5  = valid_func(valid_loader, network, criterion)\n",
    "    logger.log('[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%'.format(epoch_str, valid_a_loss, valid_a_top1, valid_a_top5))\n",
    "    #cur_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    cur_arch = search_find_best(valid_loader, network, criterion, 2)\n",
    "    genotypes[epoch] = cur_arch\n",
    "# check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies['best']:\n",
    "        valid_accuracies['best'] = valid_a_top1\n",
    "        find_best = True\n",
    "    else: find_best = False\n",
    "\n",
    "# save checkpoint\n",
    "    save_path = save_checkpoint({'epoch' : epoch + 1,\n",
    "            'args'  : deepcopy(xargs),\n",
    "            'search_model': search_model.state_dict(),\n",
    "            'w_optimizer' : w_optimizer.state_dict(),\n",
    "            'w_scheduler' : w_scheduler.state_dict(),\n",
    "            'genotypes'   : genotypes,\n",
    "            'valid_accuracies' : valid_accuracies},\n",
    "            model_base_path, logger)\n",
    "    last_info = save_checkpoint({\n",
    "      'epoch': epoch + 1,\n",
    "      'args' : deepcopy(args),\n",
    "      'last_checkpoint': save_path,\n",
    "      }, logger.path('info'), logger)\n",
    "    if find_best:\n",
    "        logger.log('<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.'.format(epoch_str, valid_a_top1))\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None: logger.log('{:}'.format(api.query_by_arch( genotypes[epoch] )))\n",
    "# measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.log('\\n' + '-'*200)\n",
    "logger.log('Pre-searching costs {:.1f} s'.format(search_time.sum))\n",
    "start_time = time.time()\n",
    "best_arch, best_acc = None, -1\n",
    "for iarch in range(xargs.select_num):\n",
    "    arch = search_model.random_genotype( True )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5  = valid_func(valid_loader, network, criterion)\n",
    "    logger.log('final evaluation [{:02d}/{:02d}] : {:} : accuracy={:.2f}%, loss={:.3f}'.format(iarch, xargs.select_num, arch, valid_a_top1, valid_a_loss))\n",
    "    if best_arch is None or best_acc < valid_a_top1:\n",
    "        best_arch, best_acc = arch, valid_a_top1\n",
    "search_time.update(time.time() - start_time)\n",
    "logger.log('RANDOM-NAS finds the best one : {:} with accuracy={:.2f}%, with {:.1f} s.'.format(best_arch, best_acc, search_time.sum))\n",
    "if api is not None: logger.log('{:}'.format( api.query_by_arch(best_arch[0]) ))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
