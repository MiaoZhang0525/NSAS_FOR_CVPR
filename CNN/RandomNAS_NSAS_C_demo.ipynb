{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20200709-234211\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "\n",
    "import genotypes\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=2, help='num of training epochs')######change the epochs\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.05, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "CIFAR_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_arch_generate():#######randomly generate architecture\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = 4####model._step\n",
    "\n",
    "    arch_gene = []\n",
    "    for i in range(n_nodes):\n",
    "        ops = np.random.choice(range(num_ops), 2)\n",
    "        nodes_in_normal = np.array([i,i+1])#########the only difference of RandomNAS_NSAS_D and RandomNAS_NSAS\n",
    "        arch_gene.extend([(ops[0],nodes_in_normal[0]), (ops[1],nodes_in_normal[1])])\n",
    "    return arch_gene  \n",
    "\n",
    "def cal_arch_dis(arch1,arch2):#calculate the distance, smaller distance more similar\n",
    "    dis=8\n",
    "    n_nodes=4######genotypes.STEPS\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        if arch1[2*i]==arch2[2*i]:\n",
    "            dis=dis-1\n",
    "        elif arch1[2*i]==arch2[2*i+1]:\n",
    "            dis=dis-1\n",
    "        if arch1[2*i+1]==arch2[2*i+1]:\n",
    "            dis=dis-1\n",
    "        elif arch1[2*i+1]==arch2[2*i]:\n",
    "            dis=dis-1                      \n",
    "    dis=dis/8\n",
    "    return dis \n",
    "\n",
    "\n",
    "\n",
    "def cal_diver_score(arch,archive):###KNN based diversity calculation\n",
    "    n=len(archive)\n",
    "    dis=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        dis[i]=cal_arch_dis(arch,archive[i])\n",
    "        \n",
    "    sort_dis=np.sort(dis)\n",
    "\n",
    "    diver_score=np.mean(sort_dis[0:10])###################################k=10 for knn\n",
    "    \n",
    "    return diver_score\n",
    " \n",
    "\n",
    "    \n",
    "def diver_arch_generate(arch_archive):############randomly genrate architecture and get the best one\n",
    "    ini_diver_score=0\n",
    "    arch_g=random_arch_generate()\n",
    "    for i in range(10):##################\n",
    "        arch=random_arch_generate()         \n",
    "        diver_score=cal_diver_score(arch,arch_archive)#########kernel metric, the samller the better\n",
    "        if diver_score>ini_diver_score:\n",
    "            arch_g=arch\n",
    "            ini_diver_score=diver_score\n",
    "            \n",
    "    return arch_g\n",
    "\n",
    "\n",
    "def diver_arch_replace(index,arch_archive,archive_recent):############randomly generate architecture to repalce\n",
    "    arch_compar=arch_archive[index]\n",
    "    a=np.arange(0,index)\n",
    "    b=np.arange(index+1,len(arch_archive))\n",
    "    index_remain=np.append(a,b)\n",
    "    \n",
    "    arch_archive_remain=[arch_archive[j] for j in index_remain]\n",
    "    \n",
    "    ini_diver_score=cal_diver_score(arch_compar,arch_archive_remain)\n",
    "    for i in range(len(archive_recent)):######################################\n",
    "        arch=archive_recent[i] \n",
    "        diver_score=cal_diver_score(arch,arch_archive_remain)\n",
    "        if diver_score>ini_diver_score:\n",
    "            arch_compar=arch\n",
    "            ini_diver_score=diver_score\n",
    "            \n",
    "    return arch_compar\n",
    "\n",
    "\n",
    "def find_similar_arch(arch,archive):##get the index of the most similar architecture\n",
    "    dis=np.zeros(len(archive))   \n",
    "    \n",
    "    for i in range(len(archive)):\n",
    "        dis[i]=cal_arch_dis(arch,archive[i])################\n",
    "\n",
    "    m=np.argsort(dis)\n",
    "    index=m[0]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch_archive_update(arch_gene,arch_archive,normal_archive_recent,reduction_archive_recent):####update architecture archive (also the constraint subset)\n",
    "    store_num=4\n",
    "    if len(arch_archive)==2*store_num:\n",
    "        ind_arch_norm_replace=find_similar_arch(arch_gene[0],arch_archive[0:len(arch_archive):2])\n",
    "        ind_arch_redu_replace=find_similar_arch(arch_gene[1],arch_archive[1:len(arch_archive):2])\n",
    "        arch_archive[2*ind_arch_norm_replace]=diver_arch_replace(ind_arch_norm_replace,arch_archive[0:len(arch_archive):2],normal_archive_recent)\n",
    "        arch_archive[2*ind_arch_redu_replace+1]=diver_arch_replace(ind_arch_redu_replace,arch_archive[1:len(arch_archive):2],reduction_archive_recent)\n",
    "        \n",
    "    else:\n",
    "        normal_arch=diver_arch_generate(arch_archive[0:len(arch_archive):2])\n",
    "        reduce_arch=diver_arch_generate(arch_archive[1:len(arch_archive):2])######greedy\n",
    "        arch_archive.append(normal_arch)\n",
    "        arch_archive.append(reduce_arch)\n",
    "    return arch_archive\n",
    "\n",
    "\n",
    "def get_weights_from_arch(arch_comb):\n",
    "    k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = model._steps\n",
    "\n",
    "    alphas_normal = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "    alphas_reduce = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(n_nodes):\n",
    "        normal1 = np.int_(arch_comb[0][2*i])\n",
    "        normal2 = np.int_(arch_comb[0][2*i+1])\n",
    "        reduce1 = np.int_(arch_comb[1][2*i])\n",
    "        reduce2 = np.int_(arch_comb[1][2*i+1])\n",
    "        alphas_normal[offset+normal1[1],normal1[0]] = 1\n",
    "        alphas_normal[offset+normal2[1],normal2[0]] = 1\n",
    "        alphas_reduce[offset+reduce1[1],reduce1[0]] = 1\n",
    "        alphas_reduce[offset+reduce2[1],reduce2[0]] = 1\n",
    "        offset += (i+2)\n",
    "\n",
    "    model_weights = [\n",
    "      alphas_normal,\n",
    "      alphas_reduce,\n",
    "    ]\n",
    "    return model_weights\n",
    "\n",
    "\n",
    "def set_model_weights(model, weights):\n",
    "    model.alphas_normal = weights[0]\n",
    "    model.alphas_reduce = weights[1]\n",
    "    model._arch_parameters = [model.alphas_normal, model.alphas_reduce]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss_archive(arch_gene,arch_archive_new,model_save,input,target,criterion):\n",
    "    loss_arch=0\n",
    "    \n",
    "    for i in range(np.int(len(arch_archive_new)/2)):\n",
    "        model_save_save=copy.deepcopy(model_save)        \n",
    "        model_weights=get_weights_from_arch(arch_archive_new[2*i:2*i+2])  \n",
    "        model_save_save=set_model_weights(model_save_save,model_weights)\n",
    "        \n",
    "        logits = model_save_save(input)        \n",
    "        loss=criterion(logits, target)\n",
    "        loss_arch=(loss_arch+loss.item())\n",
    "        del model_save_save\n",
    "    loss_archive=(loss_arch*2)/len(arch_archive_new)\n",
    "    return loss_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_queue, valid_queue, test_queue, model, architect, arch_archive,n_archive_recent,r_archive_recent, criterion, optimizer, lr):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "\n",
    "    for step, (input, target) in enumerate(train_queue):\n",
    "        #model_save=copy.deepcopy(model)\n",
    "        model.train()\n",
    "        #premodel.train\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        input_search, target_search = next(iter(valid_queue))\n",
    "        input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "        target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "             \n",
    "        arch_gene1=random_arch_generate()\n",
    "        arch_gene2=random_arch_generate()\n",
    "        arch_gene=[arch_gene1,arch_gene2]\n",
    "        model_weights=get_weights_from_arch(arch_gene)        \n",
    "        model=set_model_weights(model,model_weights)#####\n",
    "        \n",
    "        n_archive_recent.extend([arch_gene1])\n",
    "        r_archive_recent.extend([arch_gene2])\n",
    "        n_archive_recent=n_archive_recent[-50:]\n",
    "        r_archive_recent=r_archive_recent[-50:]\n",
    "        \n",
    "               \n",
    "                \n",
    "        logits = model(input)\n",
    "        \n",
    "        loss1=criterion(logits, target)\n",
    "                \n",
    "        arch_archive_new=arch_archive_update(arch_gene,arch_archive,n_archive_recent,r_archive_recent)\n",
    "        \n",
    "        loss_archive=cal_loss_archive(arch_gene,arch_archive_new,model,input,target,criterion)\n",
    "        #print(loss_archive)\n",
    "        \n",
    "        loss = 0.5*loss1+0.5*loss_archive\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "        \n",
    "        #model=set_model_weights(model,arch_param_save)###########################set back\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "    return top1.avg, objs.avg,n_archive_recent,r_archive_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 11:42:11 PM gpu device = 0\n",
      "07/09 11:42:11 PM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=32, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=2, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-EXP-20200709-234211', seed=1, train_portion=0.05, unrolled=True, weight_decay=0.0003)\n",
      "07/09 11:42:15 PM param size = 1.930618MB\n"
     ]
    }
   ],
   "source": [
    "def infer(valid_queue, model,arch_gen_compa, criterion):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    model_weights=get_weights_from_arch(arch_gen_compa)        ###########################\n",
    "    model=set_model_weights(model,model_weights)#############\n",
    "    \n",
    "    \n",
    "\n",
    "    for step, (input, target) in enumerate(valid_queue):\n",
    "        input = Variable(input, volatile=True).cuda()\n",
    "        target = Variable(target, volatile=True).cuda(async=True)\n",
    "\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        n = input.size(0)\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "  #  model=set_model_weights(model,arch_param_save)############################\n",
    "\n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "model = model.cuda()\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    args.learning_rate,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(args.train_portion * num_train))\n",
    "split_end = int(num_train)\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:split_end]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "test_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=valid_transform)\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "      test_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "      optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "architect = Architect(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 11:42:17 PM epoch 0 lr 7.000000e-03\n",
      "07/09 11:42:17 PM genotype = Genotype(normal=[('dil_conv_3x3', 1), ('dil_conv_3x3', 0), ('dil_conv_5x5', 0), ('avg_pool_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_3x3', 3), ('sep_conv_5x5', 3), ('avg_pool_3x3', 1)], normal_concat=range(2, 6), reduce=[('sep_conv_3x3', 1), ('dil_conv_5x5', 0), ('max_pool_3x3', 1), ('avg_pool_3x3', 2), ('dil_conv_5x5', 2), ('dil_conv_3x3', 3), ('skip_connect', 0), ('skip_connect', 1)], reduce_concat=range(2, 6))\n",
      "tensor([[0.1251, 0.1251, 0.1250, 0.1250, 0.1250, 0.1248, 0.1251, 0.1250],\n",
      "        [0.1251, 0.1250, 0.1249, 0.1251, 0.1251, 0.1248, 0.1252, 0.1248],\n",
      "        [0.1252, 0.1250, 0.1249, 0.1249, 0.1247, 0.1250, 0.1250, 0.1252],\n",
      "        [0.1249, 0.1250, 0.1252, 0.1248, 0.1250, 0.1251, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1251, 0.1250],\n",
      "        [0.1250, 0.1248, 0.1250, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249],\n",
      "        [0.1250, 0.1251, 0.1250, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250],\n",
      "        [0.1250, 0.1248, 0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1252],\n",
      "        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1249, 0.1252, 0.1251],\n",
      "        [0.1250, 0.1250, 0.1248, 0.1250, 0.1250, 0.1251, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1252, 0.1250, 0.1250, 0.1250, 0.1250, 0.1248],\n",
      "        [0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249],\n",
      "        [0.1250, 0.1250, 0.1249, 0.1250, 0.1251, 0.1252, 0.1248, 0.1250],\n",
      "        [0.1251, 0.1248, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248, 0.1251]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1251, 0.1247, 0.1249, 0.1250, 0.1251, 0.1251, 0.1251, 0.1251],\n",
      "        [0.1249, 0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1249, 0.1249],\n",
      "        [0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1250, 0.1249, 0.1249],\n",
      "        [0.1249, 0.1253, 0.1249, 0.1250, 0.1249, 0.1252, 0.1249, 0.1250],\n",
      "        [0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1251, 0.1248, 0.1250],\n",
      "        [0.1249, 0.1251, 0.1249, 0.1252, 0.1251, 0.1250, 0.1249, 0.1249],\n",
      "        [0.1248, 0.1250, 0.1250, 0.1249, 0.1252, 0.1251, 0.1249, 0.1249],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1248, 0.1249, 0.1249, 0.1251, 0.1252],\n",
      "        [0.1250, 0.1248, 0.1249, 0.1250, 0.1251, 0.1249, 0.1252, 0.1250],\n",
      "        [0.1248, 0.1251, 0.1250, 0.1252, 0.1249, 0.1249, 0.1251, 0.1249],\n",
      "        [0.1250, 0.1250, 0.1247, 0.1252, 0.1251, 0.1249, 0.1250, 0.1251],\n",
      "        [0.1250, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1251, 0.1247],\n",
      "        [0.1250, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1249],\n",
      "        [0.1249, 0.1250, 0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1249]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 11:42:19 PM train 000 2.396207e+00 3.125000 56.250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 11:45:13 PM train 050 2.085844e+00 22.120098 74.693626\n",
      "07/09 11:46:50 PM train_acc 24.359999\n",
      "07/09 11:46:50 PM epoch 1 lr 1.000000e-03\n",
      "07/09 11:46:50 PM genotype = Genotype(normal=[('avg_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 1), ('sep_conv_5x5', 2), ('max_pool_3x3', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 3), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 0), ('max_pool_3x3', 1), ('sep_conv_3x3', 1), ('dil_conv_3x3', 2), ('skip_connect', 2), ('sep_conv_3x3', 3), ('skip_connect', 3), ('dil_conv_5x5', 4)], reduce_concat=range(2, 6))\n",
      "tensor([[0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797]],\n",
      "       device='cuda:0')\n",
      "tensor([[0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1029, 0.1029, 0.1029, 0.2797, 0.1029, 0.1029, 0.1029, 0.1029],\n",
      "        [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797]],\n",
      "       device='cuda:0')\n",
      "07/09 11:46:54 PM train 000 1.816282e+00 21.875000 81.250000\n",
      "07/09 11:49:47 PM train 050 1.769580e+00 34.436275 85.539215\n",
      "07/09 11:51:24 PM train_acc 35.000000\n"
     ]
    }
   ],
   "source": [
    "arch_archive=[]\n",
    "arch_gen1=random_arch_generate()\n",
    "arch_gen2=random_arch_generate()\n",
    "\n",
    "\n",
    "n_archive_recent=[arch_gen1]\n",
    "r_archive_recent=[arch_gen2]\n",
    "\n",
    "record_train_acc=[]\n",
    "record_valid_acc=[]\n",
    "record_valid_acc_retrain=[]\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    logging.info('epoch %d lr %e', epoch, lr)\n",
    "\n",
    "    genotype = model.genotype()\n",
    "    logging.info('genotype = %s', genotype)\n",
    "\n",
    "    print(F.softmax(model.alphas_normal, dim=-1))\n",
    "    print(F.softmax(model.alphas_reduce, dim=-1))\n",
    "\n",
    "    # training\n",
    "    train_acc, train_obj,n_archive_recent,r_archive_recent= train(train_queue, valid_queue, test_queue, model, architect, arch_archive,n_archive_recent,r_archive_recent, criterion, optimizer, lr)\n",
    "    logging.info('train_acc %f', train_acc)\n",
    "\n",
    "    # validation                           \n",
    "                           \n",
    "\n",
    "utils.save(model, os.path.join(args.save, 'random_nsas_D_weights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n",
      "/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 11:51:25 PM valid 000 2.027170e+00 25.000000 84.375000\n",
      "07/09 11:51:33 PM valid 050 1.881227e+00 31.433825 84.865196\n",
      "07/09 11:51:43 PM valid 100 1.868398e+00 31.745049 84.839111\n",
      "07/09 11:51:53 PM valid 150 1.855147e+00 31.870861 85.513245\n",
      "07/09 11:52:02 PM valid 200 1.852250e+00 31.592039 85.665421\n",
      "07/09 11:52:10 PM valid 250 1.853777e+00 31.424303 85.657372\n",
      "07/09 11:52:18 PM valid 300 1.846670e+00 31.717192 85.890778\n",
      "07/09 11:52:26 PM valid 350 1.845104e+00 31.775284 85.897438\n",
      "07/09 11:52:34 PM valid 400 1.837930e+00 31.850063 86.034912\n",
      "07/09 11:52:42 PM valid 450 1.839026e+00 31.714247 85.871674\n",
      "07/09 11:52:50 PM valid 500 1.835132e+00 31.911177 85.896957\n",
      "07/09 11:52:58 PM valid 550 1.834542e+00 32.026997 85.860931\n",
      "07/09 11:53:06 PM valid 600 1.832987e+00 31.988352 85.971298\n",
      "07/09 11:53:14 PM valid 650 1.828364e+00 32.147659 86.093513\n",
      "07/09 11:53:22 PM valid 700 1.827963e+00 32.203995 86.073471\n",
      "07/09 11:53:30 PM valid 750 1.826005e+00 32.269474 86.139313\n",
      "07/09 11:53:38 PM valid 800 1.825133e+00 32.315075 86.165726\n",
      "07/09 11:53:46 PM valid 850 1.822816e+00 32.340630 86.214745\n",
      "07/09 11:53:54 PM valid 900 1.824714e+00 32.200333 86.175087\n",
      "07/09 11:54:02 PM valid 950 1.825335e+00 32.222660 86.133011\n",
      "07/09 11:54:10 PM valid 1000 1.826428e+00 32.155342 86.179443\n",
      "07/09 11:54:19 PM valid 1050 1.824516e+00 32.237156 86.212532\n",
      "07/09 11:54:27 PM valid 1100 1.824768e+00 32.263283 86.239784\n",
      "07/09 11:54:35 PM valid 1150 1.825161e+00 32.273567 86.251091\n",
      "07/09 11:54:43 PM valid 1200 1.823862e+00 32.264778 86.321297\n",
      "07/09 11:54:51 PM valid 1250 1.825524e+00 32.219223 86.330933\n",
      "07/09 11:54:59 PM valid 1300 1.827694e+00 32.191582 86.267776\n",
      "07/09 11:55:07 PM valid 1350 1.828335e+00 32.237694 86.188469\n",
      "07/09 11:55:15 PM valid 1400 1.827449e+00 32.249287 86.255356\n",
      "07/09 11:55:23 PM valid 1450 1.825612e+00 32.341919 86.270241\n",
      "07/09 11:55:29 PM valid 000 1.641599e+00 40.625000 87.500000\n",
      "07/09 11:55:37 PM valid 050 1.766388e+00 34.803925 86.397064\n",
      "07/09 11:55:45 PM valid 100 1.744418e+00 35.550743 87.128708\n",
      "07/09 11:55:53 PM valid 150 1.753568e+00 34.871689 86.692879\n",
      "07/09 11:56:01 PM valid 200 1.751511e+00 35.043533 86.847015\n",
      "07/09 11:56:09 PM valid 250 1.758185e+00 34.561752 86.678291\n",
      "07/09 11:56:18 PM valid 300 1.750022e+00 35.080978 86.773254\n",
      "07/09 11:56:28 PM valid 350 1.756647e+00 34.668804 86.707619\n",
      "07/09 11:56:36 PM valid 400 1.754116e+00 34.982857 86.790840\n",
      "07/09 11:56:44 PM valid 450 1.756164e+00 34.804600 86.682373\n",
      "07/09 11:56:52 PM valid 500 1.756248e+00 34.830338 86.657936\n",
      "07/09 11:57:00 PM valid 550 1.755256e+00 34.845734 86.683304\n",
      "07/09 11:57:08 PM valid 600 1.756749e+00 34.775372 86.683647\n",
      "07/09 11:57:17 PM valid 650 1.758195e+00 34.735023 86.731949\n",
      "07/09 11:57:27 PM valid 700 1.757249e+00 34.847542 86.661911\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62da6007eeba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbest_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-62da6007eeba>\u001b[0m in \u001b[0;36mrandom_selection\u001b[0;34m(valid_queue, model, criterion, rounds)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0march_gen_compa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0march_gen1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0march_gen2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0march_gen_compa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0msample_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march_gen_compa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-104b8ef5c10a>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(valid_queue, model, arch_gen_compa, criterion)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s0, s1, weights)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/model_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/model_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/NSAS_FOR_TPAMI/CNN/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yanhzhan/anaconda_y/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def random_selection(valid_queue, model, criterion, rounds=None):\n",
    "    #n_rounds = int(self.B / 7 / 1000)\n",
    "    if rounds is None:\n",
    "        n_rounds = 5\n",
    "    else:\n",
    "        n_rounds = rounds\n",
    "    best_rounds = []\n",
    "    for r in range(n_rounds):\n",
    "        sample_vals = []\n",
    "        for _ in range(10):###################set the rounds for random search\n",
    "           \n",
    "            arch_gen1=random_arch_generate()\n",
    "            arch_gen2=random_arch_generate()\n",
    "            arch_gen_compa=[arch_gen1,arch_gen2]\n",
    "            \n",
    "            valid_acc, valid_obj = infer(valid_queue, model,arch_gen_compa, criterion)\n",
    "\n",
    "            sample_vals.append((arch_gen_compa, valid_acc))\n",
    "        sample_vals = sorted(sample_vals, key=lambda x:x[1],reverse=True)\n",
    "\n",
    "        best_rounds.append(sample_vals[0])\n",
    "    return best_rounds\n",
    "\n",
    "\n",
    "\n",
    "best_rounds=random_selection(valid_queue, model, criterion, rounds=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
