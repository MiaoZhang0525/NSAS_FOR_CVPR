{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : EA_search-EXP-20191021-220816\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "\n",
    "import genotypes\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=48, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.save = 'EA_search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "CIFAR_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_arch_generate():\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = 4####model._step\n",
    "\n",
    "    arch_gene = []\n",
    "    for i in range(n_nodes):\n",
    "        ops = np.random.choice(range(num_ops), 2)\n",
    "        nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "        arch_gene.extend([(ops[0],nodes_in_normal[0]), (ops[1],nodes_in_normal[1])])\n",
    "    return arch_gene  \n",
    "\n",
    "def get_weights_from_arch(arch_comb):\n",
    "    k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = model._steps\n",
    "\n",
    "    alphas_normal = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "    alphas_reduce = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(n_nodes):\n",
    "        normal1 = np.int_(arch_comb[0][2*i])\n",
    "        normal2 = np.int_(arch_comb[0][2*i+1])\n",
    "        reduce1 = np.int_(arch_comb[1][2*i])\n",
    "        reduce2 = np.int_(arch_comb[1][2*i+1])\n",
    "        alphas_normal[offset+normal1[1],normal1[0]] = 1\n",
    "        alphas_normal[offset+normal2[1],normal2[0]] = 1\n",
    "        alphas_reduce[offset+reduce1[1],reduce1[0]] = 1\n",
    "        alphas_reduce[offset+reduce2[1],reduce2[0]] = 1\n",
    "        offset += (i+2)\n",
    "\n",
    "    model_weights = [\n",
    "      alphas_normal,\n",
    "      alphas_reduce,\n",
    "    ]\n",
    "    return model_weights\n",
    "\n",
    "\n",
    "def set_model_weights(model, weights):\n",
    "    model.alphas_normal = weights[0]\n",
    "    model.alphas_reduce = weights[1]\n",
    "    model._arch_parameters = [model.alphas_normal, model.alphas_reduce]\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train(train_queue, valid_queue, test_queue, model, architect, arch_archive, criterion, optimizer, lr):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "\n",
    "    for step, (input, target) in enumerate(train_queue):\n",
    "        #model_save=copy.deepcopy(model)\n",
    "        model.train()\n",
    "        #premodel.train\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        input_search, target_search = next(iter(valid_queue))\n",
    "        input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "        target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "        \n",
    "\n",
    "        arch_gene=random_arch_generate\n",
    "        model_weights=get_weights_from_arch(arch_gene)        \n",
    "        model=set_model_weights(model,model_weights)#####        \n",
    "        \n",
    "                       \n",
    "        logits = model(input)\n",
    "        \n",
    "        loss=criterion(logits, target)\n",
    "                \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "\n",
    "def infer(valid_queue, model,arch_gen_compa, criterion):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "    model.eval()\n",
    "    \n",
    "    model_weights=get_weights_from_arch(arch_gen_compa)        ###########################\n",
    "    model=set_model_weights(model,model_weights)#############\n",
    "    \n",
    "    \n",
    "\n",
    "    for step, (input, target) in enumerate(valid_queue):\n",
    "        input = Variable(input, volatile=True).cuda()\n",
    "        target = Variable(target, volatile=True).cuda(async=True)\n",
    "\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        n = input.size(0)\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "    valid_acc=top1.avg\n",
    "    \n",
    "    return valid_acc.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/21 10:08:16 PM gpu device = 0\n",
      "10/21 10:08:16 PM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=48, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='EA_search-EXP-20191021-220816', seed=2, train_portion=0.5, unrolled=True, weight_decay=0.0003)\n",
      "10/21 10:08:18 PM param size = 1.930618MB\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "model = model.cuda()\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    args.learning_rate,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(args.train_portion* num_train))###\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "test_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=valid_transform)\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "      test_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "      optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "architect = Architect(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained supernet model\n",
    "\n",
    "model.load_state_dict(torch.load('/data/jiahzhao/NSAS/lifelong-randomnas/cnn/CL_CNN_SUPERNET/randomNAS_CL_supernet/randomNAS_CL_constraint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EA_arch_search(model,num_pop,num_ite,num_cross,num_mutation):\n",
    "\n",
    "    def get_init_pop(num_pop,n_nodes):\n",
    "        pop=np.empty((num_pop,8*n_nodes))\n",
    "        fitness=np.zeros((num_pop,))\n",
    "        for m in range(num_pop):         \n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "            normal = []\n",
    "            reduction = []\n",
    "            for i in range(n_nodes):\n",
    "                ops = np.random.choice(range(num_ops), 4)\n",
    "                nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "                nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "                normal.extend([(ops[0],nodes_in_normal[0]), (ops[1],nodes_in_normal[1])])\n",
    "                reduction.extend([(ops[2],nodes_in_reduce[0]), (ops[3],nodes_in_reduce[1])])\n",
    "                pop[m,4*i]=ops[0]\n",
    "                pop[m,4*i+1]=nodes_in_normal[0]\n",
    "                pop[m,4*i+2]=ops[1]\n",
    "                pop[m,4*i+3]=nodes_in_normal[1]\n",
    "                pop[m,4*i+4*n_nodes]=ops[2]\n",
    "                pop[m,4*i+1+4*n_nodes]=nodes_in_reduce[0]\n",
    "                pop[m,4*i+2+4*n_nodes]=ops[3]\n",
    "                pop[m,4*i+3+4*n_nodes]=nodes_in_reduce[1]      \n",
    "            arch=[normal, reduction]\n",
    "            fitness[m,]=infer(valid_queue, model,arch, criterion)     \n",
    "        return pop,fitness\n",
    "\n",
    "\n",
    "    def corssover(pop,fitness,num_cross):\n",
    "        index=np.argsort(fitness)\n",
    "        pop_select=pop[index[0:num_cross],]\n",
    "\n",
    "\n",
    "        inde_cross=np.arange(num_cross)\n",
    "        np.random.shuffle(inde_cross)\n",
    "        pop_select=pop_select[inde_cross,]\n",
    "        pop_cross=np.empty((num_cross,pop.shape[1]))\n",
    "\n",
    "\n",
    "        for i in range(np.int(num_cross/2)):\n",
    "            cross1=pop_select[2*i,]\n",
    "            cross2=pop_select[2*i+1,]\n",
    "\n",
    "            cross_points=np.arange(4*4)##self.model.model._steps\n",
    "            np.random.shuffle(cross_points)\n",
    "            cross_points=cross_points[0:2]\n",
    "            cross_points=np.sort(cross_points)\n",
    "            p1=2*cross_points[0]\n",
    "            p2=2*cross_points[1]\n",
    "\n",
    "            cross1_=cross1\n",
    "            cross2_=cross2\n",
    "\n",
    "            cross1_[p1:p2]=cross2[p1:p2]\n",
    "            cross2_[p1:p2]=cross1[p1:p2]\n",
    "\n",
    "            pop_cross[2*i,]= cross1_       \n",
    "            pop_cross[2*i+1,]= cross2_   \n",
    "\n",
    "        return pop_cross\n",
    "\n",
    "\n",
    "    def mutation(pop,fitness,num_mutation):\n",
    "        index=np.argsort(fitness)\n",
    "        pop_select=pop[index[0:num_mutation],]\n",
    "        pop_mutation=np.empty((num_mutation,pop.shape[1]))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "\n",
    "        for i in range(num_mutation):\n",
    "            pop_mutation[i,]=pop_select[i,]\n",
    "\n",
    "            for j in range(pop.shape[1]):\n",
    "                if j>((pop.shape[1])/2-1):\n",
    "                    q=j-(pop.shape[1])/2\n",
    "                else:\n",
    "                    q=j\n",
    "                m=q//4+2\n",
    "                if np.random.rand()<0.2:#################genes with mutation probability 0.2\n",
    "                    if j%2==0:\n",
    "                        pop_mutation[i,j]=np.random.randint(num_ops)\n",
    "                    else:\n",
    "                        pop_mutation[i,j]=np.random.randint(m)            \n",
    "        return pop_mutation\n",
    "\n",
    "\n",
    "    def get_fitness(pop):\n",
    "        num_pop=pop.shape[0]\n",
    "        fitness=np.zeros((num_pop))\n",
    "        for m in range(num_pop):\n",
    "            indiv=pop[m,]\n",
    "            normal=[]\n",
    "            reduction=[]\n",
    "            for i in range(4):########self.model.model._steps\n",
    "                s1=np.int(indiv[4*i,])\n",
    "                s2=np.int(indiv[4*i+1,])\n",
    "                s3=np.int(indiv[4*i+2,])\n",
    "                s4=np.int(indiv[4*i+3,])\n",
    "                s5=np.int(indiv[4*i+16,])\n",
    "                s6=np.int(indiv[4*i+1+16,])\n",
    "                s7=np.int(indiv[4*i+2+16,])\n",
    "                s8=np.int(indiv[4*i+3+16,])\n",
    "                normal.extend([(s1,s2), (s3,s4)])\n",
    "                reduction.extend([(s5,s6), (s7,s8)]) \n",
    "            arch=[normal, reduction]\n",
    "            fitness[m,]=infer(valid_queue, model,arch, criterion)\n",
    "\n",
    "        return fitness\n",
    "    \n",
    "    def regulize_pop(pop):\n",
    "        num_pop=pop.shape[0]\n",
    "        fitness=np.zeros((num_pop))\n",
    "        for m in range(num_pop):\n",
    "            \n",
    "            for j in range(8):\n",
    "                if j>3:\n",
    "                    kk=j-4\n",
    "                else:\n",
    "                    kk=j\n",
    "                while pop[m][4*j+1]==pop[m][4*j+3]:                   \n",
    "                    pop[m][4*j+3]=np.random.randint(kk+2)       \n",
    "        return pop   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    n_nodes = 4######self.model.model._steps    \n",
    "\n",
    "    pop,fitness=get_init_pop(num_pop,n_nodes)\n",
    "\n",
    "    for it in range(num_ite):\n",
    "        pop_cross=corssover(pop,fitness,num_cross)\n",
    "        fitness_cross=get_fitness(pop_cross)\n",
    "        pop_mutate=mutation(pop,fitness,num_mutation)\n",
    "        fitness_mutate=get_fitness(pop_mutate) \n",
    "        pop_comb=np.concatenate((pop,pop_cross,pop_mutate),axis=0)\n",
    "        fitness_comb=np.concatenate((fitness,fitness_cross,fitness_mutate),axis=0)\n",
    "        index=np.argsort(fitness_comb)\n",
    "        pop_comb=pop_comb[index,]\n",
    "        pop=pop_comb[0:num_pop,]\n",
    "        fitness=fitness_comb[0:num_pop,]\n",
    "\n",
    "    index=np.argsort(fitness)\n",
    "    \n",
    "    best_arch=[]\n",
    "    \n",
    "    for b in range(2):\n",
    "        indi_final=pop[index[b],]\n",
    "\n",
    "        normal = []\n",
    "        normal_struc=[]\n",
    "        reduction = []\n",
    "        reduction_struc=[]\n",
    "        for i in range(4):####self.model.model._steps\n",
    "\n",
    "            s1=np.int(indi_final[4*i,])\n",
    "            s2=np.int(indi_final[4*i+1,])\n",
    "            s3=np.int(indi_final[4*i+2,])\n",
    "            s4=np.int(indi_final[4*i+3,])\n",
    "            s5=np.int(indi_final[4*i+16,])\n",
    "            s6=np.int(indi_final[4*i+1+16,])\n",
    "            s7=np.int(indi_final[4*i+2+16,])\n",
    "            s8=np.int(indi_final[4*i+3+16,])\n",
    "            normal.extend([(s1,s2), (s3,s4)])\n",
    "            normal_struc.append((genotypes.PRIMITIVES[s1],s2))\n",
    "            normal_struc.append((genotypes.PRIMITIVES[s3],s4))\n",
    "\n",
    "            reduction.extend([(s5,s6), (s7,s8)])            \n",
    "            reduction_struc.append((genotypes.PRIMITIVES[s5],s6))\n",
    "            reduction_struc.append((genotypes.PRIMITIVES[s7],s8))\n",
    "\n",
    "        concat = range(2, 6)\n",
    "        genotype = genotypes.Genotype(normal=normal_struc, normal_concat=concat,reduce=reduction_struc, reduce_concat=concat)\n",
    "        best_arch.append(genotype)\n",
    "\n",
    "    return best_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=EA_arch_search(model,100,50,60,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
